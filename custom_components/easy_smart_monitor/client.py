import aiohttp
import asyncio
import logging
import os
import json
import gzip
from datetime import datetime
from typing import List, Dict, Any, Optional

# Importações oficiais do Core para persistência atômica segura
from homeassistant.helpers.json import save_json
from homeassistant.util.json import load_json
from homeassistant.core import HomeAssistant

from .const import (
    DOMAIN,
    STORAGE_FILE,
    TEST_MODE,
    MAX_RETRIES,
    RETRY_DELAY,
    HEADERS,
    DEFAULT_PING_HOST,
    NAME
)

_LOGGER = logging.getLogger(__name__)

class EasySmartClient:
    """
    Cliente API exaustivo com persistência nativa HA,
    gestão de concorrência e lógica de re-tentativa (Retry).
    """

    def __init__(self, host: str, username: str, password: str, session: aiohttp.ClientSession, hass: HomeAssistant):
        """Inicializa o cliente com caminhos de armazenamento protegidos pelo Core do HA."""
        self.host = host.rstrip("/")
        self.username = username
        self.password = password
        self.session = session
        self.hass = hass
        self.queue: List[Dict[str, Any]] = []
        self.token: Optional[str] = None

        # Define o caminho absoluto dentro de /config/.storage/
        self.storage_path = hass.config.path(".storage", STORAGE_FILE)

        # Lock de concorrência para evitar que múltiplos envios ocorram simultaneamente
        self._lock = asyncio.Lock()

        # Registro de última comunicação
        self._last_communication_time: Optional[datetime] = None

        _LOGGER.debug("Cliente %s inicializado. Caminho de armazenamento: %s", NAME, self.storage_path)

    def _count_total_sensors(self) -> int:
        """
        Conta o total de sensores na fila.
        A fila está agrupada por equipamento, então precisamos somar todos os sensores.
        """
        total = 0
        for item in self.queue:
            sensors = item.get("sensor", [])
            if isinstance(sensors, list):
                total += len(sensors)
            elif sensors:  # Se for um único sensor (formato antigo)
                total += 1
        return total

    async def authenticate(self) -> bool:
        """
        Realiza a autenticação como dispositivo IoT e obtém o Bearer Token.
        Usa o endpoint específico para dispositivos: /api/v1/auth/device/login
        """
        if TEST_MODE:
            _LOGGER.info("MODO TESTE: Simulando sucesso na autenticação.")
            self.token = "token_teste_v11_estavel"
            self._last_communication_time = datetime.now()
            self.hass.add_job(self._save_queue_to_disk)
            return True

        # Endpoint específico para autenticação de dispositivos IoT
        url = f"{self.host}/api/v1/auth/device/login"
        payload = {
            "username": self.username,
            "password": self.password
        }

        _LOGGER.debug("Enviando requisição de login de dispositivo para: %s", url)
        try:
            async with self.session.post(url, json=payload, timeout=15) as response:
                if response.status == 200:
                    self._last_communication_time = datetime.now()
                    data = await response.json()
                    self.token = data.get("access_token")
                    _LOGGER.info("Autenticação de dispositivo na API %s realizada com sucesso.", NAME)
                    # Persiste a metadata de sucesso
                    self.hass.add_job(self._save_queue_to_disk)
                    return True
                
                # Tratar erros específicos de status do usuário
                if response.status == 403:
                    try:
                        error_data = await response.json()
                        error_msg = error_data.get("message", "Acesso negado")
                        _LOGGER.error(
                            "Acesso negado na autenticação. Status: %s. Mensagem: %s",
                            response.status, error_msg
                        )
                    except:
                        _LOGGER.error(
                            "Acesso negado na autenticação. Status: %s. Verifique se o usuário está ativo e não bloqueado.",
                            response.status
                        )

                _LOGGER.error(
                    "Falha na autenticação. Status: %s. Verifique se as credenciais estão corretas e o usuário é do tipo 'device'.",
                    response.status
                )
        except (asyncio.TimeoutError, aiohttp.ClientError) as e:
            # Não logamos erro aqui pois o Coordinator tratará a lógica de diagnóstico
            pass
        except Exception as e:
            _LOGGER.error("Erro inesperado no processo de login: %s", e)

        return False

    async def check_internet(self) -> bool:
        """Verifica se há conectividade com a internet básica."""
        try:
            # Tenta uma conexão simples na porta 53 (DNS) ou 80 do host de ping
            # Usando timeout curto para ser rápido
            import socket
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(
                None, 
                lambda: socket.create_connection((DEFAULT_PING_HOST, 53), timeout=3)
            )
            return True
        except Exception:
            return False

    def add_to_queue(self, equip_uuid: str, header: dict, sensor: dict):
        """Adiciona dados à fila agrupando por equipamento para evitar duplicidade de cabeçalho."""
        # Procura se já existe uma entrada para este equipamento na fila atual
        for item in self.queue:
            if item.get("equip_uuid") == equip_uuid:
                # Se encontrou, adiciona o sensor à lista existente
                if "sensor" not in item:
                    item["sensor"] = []
                elif not isinstance(item["sensor"], list):
                    # Caso de migração de formato antigo
                    item["sensor"] = [item["sensor"]]
                
                item["sensor"].append(sensor)
                # Atualiza o cabeçalho com os dados mais recentes (caso status tenha mudado)
                item.update(header)
                self.hass.add_job(self._save_queue_to_disk)
                return

        # Se não encontrou, cria um novo item na fila
        new_item = header.copy()
        new_item["sensor"] = [sensor]
        self.queue.append(new_item)
        _LOGGER.debug("Novo equipamento adicionado à fila. Itens: %s", len(self.queue))

        # Agenda a gravação para o disco de forma assíncrona (Thread-safe)
        self.hass.add_job(self._save_queue_to_disk)

    async def sync_queue(self) -> bool:
        """
        Sincroniza a fila acumulada com o servidor remoto.
        Implementa Retry, Renovação de Token automático e tratamento de erros de rede.
        """
        if not self.queue:
            # Heartbeat: Se a fila estiver vazia, realizamos uma comunicação mínima
            # apenas para validar a conexão e atualizar o timestamp de "Última Sincronização".
            # Isso garante que o usuário veja que o intervalo está sendo respeitado.
            _LOGGER.debug("Fila vazia. Iniciando heartbeat para manter conexão viva.")
            return await self.authenticate()

        # O Lock garante que se um ciclo de rede demorar, o próximo não atropele o atual
        async with self._lock:
            if TEST_MODE:
                total_sensors = self._count_total_sensors()
                _LOGGER.info("MODO TESTE: Simulação de envio bulk de %s sensores concluída.", total_sensors)
                self._last_communication_time = datetime.now()
                self.queue.clear()
                self._save_queue_to_disk()
                return True

            # Se não temos token, tentamos autenticar antes de prosseguir
            if not self.token and not await self.authenticate():
                _LOGGER.warning("Sincronização cancelada: Falha crítica na autenticação.")
                return False

            attempts = 0
            while attempts < MAX_RETRIES:
                try:
                    # Usar endpoint v1 da API (compatível com /api/telemetria/bulk)
                    url = f"{self.host}/api/v1/telemetry/bulk"
                    auth_headers = {
                        **HEADERS,
                        "Authorization": f"Bearer {self.token}"
                    }

                    _LOGGER.debug("Tentativa de envio bulk %s/%s para %s", attempts + 1, MAX_RETRIES, url)

                    # Comprime os dados com GZIP para reduzir tamanho (70-85% de redução)
                    json_data = json.dumps(self.queue).encode('utf-8')
                    compressed_data = gzip.compress(json_data, compresslevel=6)
                    
                    # Adiciona header indicando que os dados estão comprimidos
                    compressed_headers = {
                        **auth_headers,
                        "Content-Encoding": "gzip",
                        "Content-Type": "application/json"
                    }
                    
                    original_size = len(json_data)
                    compressed_size = len(compressed_data)
                    compression_ratio = (1 - compressed_size / original_size) * 100 if original_size > 0 else 0
                    
                    _LOGGER.debug(
                        "Dados comprimidos: %s bytes → %s bytes (%.1f%% reduzido)",
                        original_size, compressed_size, compression_ratio
                    )

                    async with self.session.post(
                        url,
                        data=compressed_data,
                        headers=compressed_headers,
                        timeout=25
                    ) as response:
                        if response.status in [200, 201]:
                            self._last_communication_time = datetime.now()
                            total_sensors = self._count_total_sensors()
                            _LOGGER.info("Sucesso! %s sensores de telemetria enviados para a API central.", total_sensors)
                            self.queue.clear()
                            self._save_queue_to_disk()
                            return True

                        if response.status == 401:
                            _LOGGER.warning("Token expirado (401). Tentando renovação de credenciais...")
                            if await self.authenticate():
                                attempts += 1
                                continue # Tenta o envio novamente com o novo token

                        _LOGGER.warning(
                            "Resposta negativa da API (Status %s). Tentativa %s/%s",
                            response.status, attempts + 1, MAX_RETRIES
                        )

                except (aiohttp.ClientError, asyncio.TimeoutError) as e:
                    _LOGGER.warning(
                        "Falha de rede na comunicação com a API: %s. Aguardando %ss para re-tentativa...",
                        e, RETRY_DELAY
                    )

                attempts += 1
                if attempts < MAX_RETRIES:
                    await asyncio.sleep(RETRY_DELAY)

            _LOGGER.error("Falha persistente na sincronização após %s tentativas. Dados mantidos para o próximo ciclo.", MAX_RETRIES)
            return False

    def _save_queue_to_disk(self):
        """
        Gravação robusta da fila e metadata usando helpers oficiais do HA.
        """
        try:
            storage_dir = os.path.dirname(self.storage_path)
            if not os.path.exists(storage_dir):
                os.makedirs(storage_dir, exist_ok=True)

            # Estrutura de dados persistente
            data_to_save = {
                "queue": self.queue,
                "api_ultima_comunicacao": self._last_communication_time.isoformat() if self._last_communication_time else None
            }

            # save_json do HA faz a escrita atômica internamente
            save_json(self.storage_path, data_to_save)
            _LOGGER.debug("Estado da fila e metadata persistidos com sucesso.")
        except Exception as e:
            _LOGGER.error("Erro crítico ao salvar dados no disco: %s", e)

    async def load_queue_from_disk(self):
        """Carrega a fila e metadata persistidas durante o boot."""
        if not os.path.exists(self.storage_path):
            return

        try:
            data = await self.hass.async_add_executor_job(load_json, self.storage_path)

            if isinstance(data, dict):
                # Novo formato (v1.3.0+)
                self.queue = data.get("queue", [])
                last_comm_str = data.get("api_ultima_comunicacao") or data.get("last_communication")
                if last_comm_str:
                    self._last_communication_time = datetime.fromisoformat(last_comm_str)
                total_sensors = self._count_total_sensors()
                _LOGGER.info("Persistência carregada: %s sensores e última comunicação restaurada.", total_sensors)
            elif isinstance(data, list):
                # Legado (v1.2.0-)
                self.queue = data
                total_sensors = self._count_total_sensors()
                _LOGGER.info("Persistência legada carregada: %s sensores restaurados.", total_sensors)
            
        except Exception as e:
            _LOGGER.error("Falha ao carregar persistência local: %s", e)
            if os.path.exists(self.storage_path):
                backup_name = f"{self.storage_path}.corrupt_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                os.rename(self.storage_path, backup_name)

    def get_diagnostics(self) -> Dict[str, Any]:
        """Extrai métricas de saúde interna para os sensores de diagnóstico."""
        return {
            "queue_size": self._count_total_sensors(),  # Total de sensores, não equipamentos
            "is_authenticated": self.token is not None,
            "api_host": self.host,
            "test_mode": TEST_MODE,
            "storage_location": self.storage_path,
            "last_communication": self._last_communication_time.strftime("%d/%m/%Y %H:%M:%S") if self._last_communication_time else "Aguardando..."
        }